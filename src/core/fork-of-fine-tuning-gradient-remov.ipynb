{"cells":[{"cell_type":"code","execution_count":11,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-12T16:42:40.214699Z","iopub.status.busy":"2024-09-12T16:42:40.214322Z","iopub.status.idle":"2024-09-12T16:43:41.602264Z","shell.execute_reply":"2024-09-12T16:43:41.601013Z","shell.execute_reply.started":"2024-09-12T16:42:40.214659Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/huggingface/transformers.git@main\n","  Cloning https://github.com/huggingface/transformers.git (to revision main) to /tmp/pip-req-build-uwr8g596\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-uwr8g596\n","  Resolved https://github.com/huggingface/transformers.git to commit 2f611d30d972c6e96759742f0b1217c442601f52\n","  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hCollecting git+https://github.com/huggingface/peft.git\n","  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-4kbinba0\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-4kbinba0\n","  Resolved https://github.com/huggingface/peft.git to commit 214f891cd26a399cc69b24f9aa54bcf6eb0aa287\n","  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (0.24.7)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (2023.6.3)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (0.4.5)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (4.66.1)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.12.1.dev0) (5.9.3)\n","Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.12.1.dev0) (2.0.0)\n","Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.12.1.dev0) (0.22.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (2023.9.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (4.6.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.45.0.dev0) (3.0.9)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.12.1.dev0) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.12.1.dev0) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.12.1.dev0) (3.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.12.1.dev0) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.12.1.dev0) (1.3.0)\n"]}],"source":["!pip install bitsandbytes datasets accelerate>0.26.0 loralib\n","!pip install git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git\n","# !pip install datasets transformers\n","# !pip install transformers datasets evaluate"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-09-12T16:45:57.587131Z","iopub.status.busy":"2024-09-12T16:45:57.586126Z","iopub.status.idle":"2024-09-12T16:45:57.738274Z","shell.execute_reply":"2024-09-12T16:45:57.737378Z","shell.execute_reply.started":"2024-09-12T16:45:57.587091Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["from huggingface_hub import login\n","\n","# !python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('7dfdc3694ead1a038bfa1cbe7bbe7946c722635d')\"\n","login(token=\"YOUR TOKEN\")"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-09-12T16:45:58.731553Z","iopub.status.busy":"2024-09-12T16:45:58.730926Z","iopub.status.idle":"2024-09-12T16:45:58.740498Z","shell.execute_reply":"2024-09-12T16:45:58.739539Z","shell.execute_reply.started":"2024-09-12T16:45:58.731522Z"},"trusted":true},"outputs":[],"source":["# Used for multi-gpu\n","local_rank = -1\n","per_device_train_batch_size = 4\n","per_device_eval_batch_size = 4\n","gradient_accumulation_steps = 1\n","learning_rate = 2e-4\n","max_grad_norm = 0.3\n","weight_decay = 0.001\n","lora_alpha = 16\n","lora_dropout = 0.1\n","lora_r = 64\n","max_seq_length = None\n","\n","model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n","\n","# Fine-tuned model name\n","new_model = \"llama-2-7b-33M-dental-100-qa\"\n","\n","# The instruction dataset to use\n","dataset_name = \"/kaggle/input/llama2-dental-no-context\"\n","\n","# Activate 4-bit precision base model loading\n","use_4bit = True\n","\n","# Activate nested quantization for 4-bit base models\n","use_nested_quant = False\n","\n","# Compute dtype for 4-bit base models\n","bnb_4bit_compute_dtype = \"float16\"\n","\n","# Quantization type (fp4 or nf4)\n","bnb_4bit_quant_type = \"nf4\"\n","\n","# Number of training epochs\n","num_train_epochs = 2\n","\n","# Enable fp16 training, (bf16 to True with an A100)\n","fp16 = False\n","\n","# Enable bf16 training\n","bf16 = False\n","\n","# Use packing dataset creating\n","packing = False\n","\n","# Enable gradient checkpointing\n","gradient_checkpointing = True\n","\n","# Optimizer to use, original is paged_adamw_32bit\n","optim = \"paged_adamw_32bit\"\n","\n","# Learning rate schedule (constant a bit better than cosine, and has advantage for analysis)\n","lr_scheduler_type = \"cosine\"\n","\n","# Number of optimizer update steps, 10K original, 20 for demo purposes\n","max_steps = -1\n","\n","# Fraction of steps to do a warmup for\n","warmup_ratio = 0.03\n","\n","# Group sequences into batches with same length (saves memory and speeds up training considerably)\n","group_by_length = True\n","\n","# Save checkpoint every X updates steps\n","save_steps = 10\n","\n","# Log every X updates steps\n","logging_steps = 1\n","\n","# The output directory where the model predictions and checkpoints will be written\n","output_dir = \"./results\"\n","\n","# Load the entire model on the GPU 0\n","device_map = {\"\": 0}\n","\n","# Visualize training\n","report_to = \"tensorboard\"\n","\n","# Tensorboard logs\n","tb_log_dir = \"./results/logs\""]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-09-12T16:45:59.672548Z","iopub.status.busy":"2024-09-12T16:45:59.672181Z","iopub.status.idle":"2024-09-12T16:46:00.038831Z","shell.execute_reply":"2024-09-12T16:46:00.037505Z","shell.execute_reply.started":"2024-09-12T16:45:59.672518Z"},"trusted":true},"outputs":[{"ename":"ImportError","evalue":"Using `bitsandbytes` 4-bit quantization requires Accelerate: `pip install 'accelerate>=0.26.0'`","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 32\u001b[0m\n\u001b[1;32m     22\u001b[0m compute_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(torch, bnb_4bit_compute_dtype)\n\u001b[1;32m     25\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m     26\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39muse_4bit,\n\u001b[1;32m     27\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39mbnb_4bit_quant_type,\n\u001b[1;32m     28\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mcompute_dtype,\n\u001b[1;32m     29\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39muse_nested_quant,\n\u001b[1;32m     30\u001b[0m )\n\u001b[0;32m---> 32\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-2-7b-chat-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:557\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    556\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3414\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3411\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3414\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3417\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3418\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:71\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo GPU found. A GPU is needed for quantization.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 4-bit quantization requires Accelerate: `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m     )\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bitsandbytes_available():\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m     )\n","\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 4-bit quantization requires Accelerate: `pip install 'accelerate>=0.26.0'`"]}],"source":["#Setup the model\n","import os\n","import torch\n","import torch.nn as nn\n","import bitsandbytes as bnb\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","\n","\n","# Compute dtype for 4-bit base models\n","bnb_4bit_compute_dtype = \"float16\"\n","\n","\n","# Load tokenizer and model with QLoRA configuration\n","compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=use_4bit,\n","    bnb_4bit_quant_type=bnb_4bit_quant_type,\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=use_nested_quant,\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=\"cuda\"\n",")\n","tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-12T16:35:04.842492Z","iopub.status.idle":"2024-09-12T16:35:04.842858Z","shell.execute_reply":"2024-09-12T16:35:04.842703Z","shell.execute_reply.started":"2024-09-12T16:35:04.842686Z"},"trusted":true},"outputs":[],"source":["for param in model.parameters():\n","  param.requires_grad = False  # freeze the model - train adapters later\n","  if param.ndim == 1:\n","    # cast the small parameters (e.g. layernorm) to fp32 for stability\n","    param.data = param.data.to(torch.float16) #reduced to 16bit\n","\n","model.gradient_checkpointing_enable()  # reduce number of stored activations\n","model.enable_input_require_grads()\n","\n","class CastOutputToFloat(nn.Sequential):\n","  def forward(self, x): return super().forward(x).to(torch.float16)\n","model.lm_head = CastOutputToFloat(model.lm_head)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-12T16:35:04.844392Z","iopub.status.idle":"2024-09-12T16:35:04.844870Z","shell.execute_reply":"2024-09-12T16:35:04.844632Z","shell.execute_reply.started":"2024-09-12T16:35:04.844610Z"},"trusted":true},"outputs":[],"source":["import torch.nn as nn\n","import torch\n","\n","def freeze_model_parameters(model):\n","    \"\"\"Freezes all the parameters of the model.\"\"\"\n","    for param in model.parameters():\n","        param.requires_grad = False\n","\n","def cast_small_parameters_to_fp16(model):\n","    \"\"\"Casts small parameters (like those in layer normalization) to fp16.\"\"\"\n","    for param in model.parameters():\n","        if param.ndim == 1:\n","            param.data = param.data.to(torch.float16)\n","\n","class CastOutputToFloat(nn.Module):\n","    \"\"\"A module wrapper that casts the output of the module to float32.\"\"\"\n","    def __init__(self, module):\n","        super(CastOutputToFloat, self).__init__()\n","        self.module = module\n","    \n","    def forward(self, x):\n","        return self.module(x).to(torch.float32)\n","\n","def improve_model(model):\n","    # Freeze all model parameters\n","    freeze_model_parameters(model)\n","    \n","    # Cast small parameters to 16-bit\n","    cast_small_parameters_to_fp16(model)\n","\n","    # Enable gradient checkpointing\n","    if hasattr(model, 'gradient_checkpointing_enable'):\n","        model.gradient_checkpointing_enable()\n","    \n","    # Enable input gradients (if applicable)\n","    if hasattr(model, 'enable_input_require_grads'):\n","        model.enable_input_require_grads()\n","\n","    # Ensure the output of lm_head is in float32\n","    model.lm_head = CastOutputToFloat(model.lm_head)\n","\n","# Assuming your model is loaded as 'model'\n","improve_model(model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-12T16:35:04.846272Z","iopub.status.idle":"2024-09-12T16:35:04.846741Z","shell.execute_reply":"2024-09-12T16:35:04.846505Z","shell.execute_reply.started":"2024-09-12T16:35:04.846484Z"},"trusted":true},"outputs":[],"source":["def print_trainable_parameters(model):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for _, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(\n","        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-12T16:35:04.848261Z","iopub.status.idle":"2024-09-12T16:35:04.848735Z","shell.execute_reply":"2024-09-12T16:35:04.848499Z","shell.execute_reply.started":"2024-09-12T16:35:04.848477Z"},"trusted":true},"outputs":[],"source":["from peft import LoraConfig, get_peft_model \n","\n","config = LoraConfig(\n","        lora_alpha=lora_alpha,\n","        lora_dropout=lora_dropout,\n","        r=lora_r,\n","        bias=\"none\",\n","        task_type=\"CAUSAL_LM\",\n","    )\n","\n","our_model = get_peft_model(model, config)\n","print_trainable_parameters(our_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-12T16:35:04.851528Z","iopub.status.idle":"2024-09-12T16:35:04.851875Z","shell.execute_reply":"2024-09-12T16:35:04.851727Z","shell.execute_reply.started":"2024-09-12T16:35:04.851711Z"},"trusted":true},"outputs":[],"source":["tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-12T16:35:04.853019Z","iopub.status.idle":"2024-09-12T16:35:04.853344Z","shell.execute_reply":"2024-09-12T16:35:04.853199Z","shell.execute_reply.started":"2024-09-12T16:35:04.853184Z"},"trusted":true},"outputs":[],"source":["from datasets import load_dataset, Dataset\n","from sklearn.model_selection import train_test_split\n","\n","def format_dolly(sample):\n","#     print(\"dolly data: \", sample)\n","    instruction = f\"<s>[INST] {sample['question']}\"\n","#     context = f\"<<SYS>>{sample['context']}<<SYS>>\" if len(sample[\"context\"]) > 0 else None\n","    response = f\" [/INST] {sample['answer']}\"\n","    # join all the parts together\n","    prompt = \"\".join([i for i in [instruction, response] if i is not None])\n","#     print(\"prompt data:\", prompt)\n","    return prompt\n","\n","# template dataset to add prompt to each sample\n","def template_dataset(sample):\n","    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n","#     print(\"updated sample: \", sample)\n","    return sample\n","\n","# apply prompt template per sample\n","dataset = load_dataset(dataset_name, split=\"train\")\n","\n","# Shuffle the dataset\n","dataset_shuffled = dataset.shuffle(seed=42)\n","\n","# Select the first 50 rows from the shuffled dataset, comment if you want 15k\n","dataset = dataset_shuffled.select(range(100))\n","# split the test eval data\n","\n","print(dataset.features)\n","dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n","\n","train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n","train_data = Dataset.from_dict(train_data)\n","test_data = Dataset.from_dict(test_data)\n","# dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T06:15:41.29734Z","iopub.status.busy":"2023-10-23T06:15:41.296519Z","iopub.status.idle":"2023-10-23T06:15:41.305171Z","shell.execute_reply":"2023-10-23T06:15:41.304222Z","shell.execute_reply.started":"2023-10-23T06:15:41.297299Z"},"trusted":true},"outputs":[],"source":["dataset[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T06:15:43.351938Z","iopub.status.busy":"2023-10-23T06:15:43.351557Z","iopub.status.idle":"2023-10-23T06:15:43.358263Z","shell.execute_reply":"2023-10-23T06:15:43.357384Z","shell.execute_reply.started":"2023-10-23T06:15:43.351907Z"},"trusted":true},"outputs":[],"source":["test_data[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T06:16:52.61257Z","iopub.status.busy":"2023-10-23T06:16:52.612189Z","iopub.status.idle":"2023-10-23T06:16:52.690391Z","shell.execute_reply":"2023-10-23T06:16:52.689509Z","shell.execute_reply.started":"2023-10-23T06:16:52.612542Z"},"trusted":true},"outputs":[],"source":["device = device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","our_model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T06:16:54.924586Z","iopub.status.busy":"2023-10-23T06:16:54.923738Z","iopub.status.idle":"2023-10-23T06:17:46.347261Z","shell.execute_reply":"2023-10-23T06:17:46.346308Z","shell.execute_reply.started":"2023-10-23T06:16:54.924555Z"},"trusted":true},"outputs":[],"source":["\n","# logging.set_verbosity(logging.CRITICAL)\n","\n","# Run text generation pipeline with our next model\n","prompt = \"What potential drug interactions are associated with abacavir in dental practice?\"\n","# Ignore warnings\"\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=1024)\n","result = pipe(f\"<s>[INST]{prompt} [/INST]\")\n","print(result[0]['generated_text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T06:17:46.349969Z","iopub.status.busy":"2023-10-23T06:17:46.349654Z","iopub.status.idle":"2023-10-23T06:17:59.110144Z","shell.execute_reply":"2023-10-23T06:17:59.109085Z","shell.execute_reply.started":"2023-10-23T06:17:46.349942Z"},"trusted":true},"outputs":[],"source":["!pip install trl"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T06:18:20.629133Z","iopub.status.busy":"2023-10-23T06:18:20.628643Z","iopub.status.idle":"2023-10-23T06:20:35.771158Z","shell.execute_reply":"2023-10-23T06:20:35.770079Z","shell.execute_reply.started":"2023-10-23T06:18:20.629091Z"},"trusted":true},"outputs":[],"source":["from transformers import Trainer\n","from transformers import DataCollatorForLanguageModeling\n","from trl import SFTTrainer\n","\n","\n","data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n","\n","\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    optim=optim,\n","    save_steps=save_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=learning_rate,\n","    fp16=fp16,\n","    bf16=bf16,\n","    max_grad_norm=max_grad_norm,\n","    max_steps=max_steps,\n","    warmup_ratio=warmup_ratio,\n","    group_by_length=group_by_length,\n","    lr_scheduler_type=lr_scheduler_type,\n","    report_to=\"tensorboard\"\n",")\n","\n","trainer = SFTTrainer(\n","    model=our_model,\n","    train_dataset=train_data,\n","    peft_config=config,\n","    dataset_text_field=\"text\",\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing=packing,\n","    eval_dataset=test_data,\n",")\n","\n","trainer.train()\n","trainer.model.save_pretrained(output_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-24T20:51:53.051412Z","iopub.status.busy":"2023-10-24T20:51:53.051169Z","iopub.status.idle":"2023-10-24T20:53:00.747248Z","shell.execute_reply":"2023-10-24T20:53:00.746024Z","shell.execute_reply.started":"2023-10-24T20:51:53.051392Z"},"trusted":true},"outputs":[],"source":["!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 guardrail-ml==0.0.12 tensorboard\n","!apt-get -qq install poppler-utils tesseract-ocr\n","!pip install -q unstructured[\"local-inference\"]==0.7.4 pillow"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-24T20:53:00.750641Z","iopub.status.busy":"2023-10-24T20:53:00.750323Z","iopub.status.idle":"2023-10-24T20:53:17.463301Z","shell.execute_reply":"2023-10-24T20:53:17.462516Z","shell.execute_reply.started":"2023-10-24T20:53:00.750614Z"},"trusted":true},"outputs":[],"source":["import os\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft import LoraConfig, PeftModel, get_peft_model\n","from trl import SFTTrainer\n","from guardrail.client import (\n","    run_metrics,\n","    run_simple_metrics,\n","    create_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-24T20:53:17.465422Z","iopub.status.busy":"2023-10-24T20:53:17.464629Z","iopub.status.idle":"2023-10-24T20:53:17.475675Z","shell.execute_reply":"2023-10-24T20:53:17.474657Z","shell.execute_reply.started":"2023-10-24T20:53:17.465385Z"},"trusted":true},"outputs":[],"source":["def text_gen_eval_wrapper(model, tokenizer, prompt, model_id=1, show_metrics=True, temp=0.7, max_length=200):\n","    \"\"\"\n","    A wrapper function for inferencing, evaluating, and logging text generation pipeline.\n","\n","    Parameters:\n","        model (str or object): The model name or the initialized text generation model.\n","        tokenizer (str or object): The tokenizer name or the initialized tokenizer for the model.\n","        prompt (str): The input prompt text for text generation.\n","        model_id (int, optional): An identifier for the model. Defaults to 1.\n","        show_metrics (bool, optional): Whether to calculate and show evaluation metrics.\n","                                       Defaults to True.\n","        max_length (int, optional): The maximum length of the generated text sequence.\n","                                    Defaults to 200.\n","\n","    Returns:\n","        generated_text (str): The generated text by the model.\n","        metrics (dict): Evaluation metrics for the generated text (if show_metrics is True).\n","    \"\"\"\n","    # Suppress Hugging Face pipeline logging\n","    logging.set_verbosity(logging.CRITICAL)\n","\n","    # Initialize the pipeline\n","    pipe = pipeline(task=\"text-generation\",\n","                    model=model,\n","                    tokenizer=tokenizer,\n","                    max_length=max_length,\n","                    do_sample=True,\n","                    temperature=temp)\n","\n","#     # Generate text using the pipeline\n","#     pipe = pipeline(task=\"text-generation\",\n","#                     model=model,\n","#                     tokenizer=tokenizer,\n","#                     max_length=1000)\n","    context = \"You are a dental chat bot. Your name is DentAI. You are talking to a patient. Your role comes before the doctor. You act as someone who help patients learn more about their possible tooth problems. You will only answer questions related to tooth problems. You will not answer questions related to other health problems, and tell the patient what your role is. only try to be in the dental domain. you should also tell them that you are not an expert, and they should seek doctor if it gets worse.\"\n","    result = pipe(f\"<s>[INST]<<<SYS>> {context} <<SYS>> {prompt} [/INST]\")\n","    generated_text = result[0]['generated_text']\n","#     print(\"generated text\", generated_text)\n","    # Find the index of \"### Assistant\" in the generated text\n","    index = generated_text.find(\"[/INST] \")\n","#     print(\"index: \", index)\n","    if index != -1:\n","        # Extract the substring after \"### Assistant\"\n","        substring_after_assistant = generated_text[index + len(\"[/INST] \"):].strip()\n","    else:\n","        # If \"### Assistant\" is not found, use the entire generated text\n","#         substring_after_assistant = generated_text.strip()\n","        substring_after_assistant = generated_text\n","\n","    if show_metrics:\n","        # Calculate evaluation metrics\n","        metrics = run_metrics(substring_after_assistant, prompt, model_id)\n","\n","        return substring_after_assistant, metrics\n","    else:\n","        return substring_after_assistant"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T06:32:16.452564Z","iopub.status.busy":"2023-10-23T06:32:16.452141Z","iopub.status.idle":"2023-10-23T06:33:08.487798Z","shell.execute_reply":"2023-10-23T06:33:08.48679Z","shell.execute_reply.started":"2023-10-23T06:32:16.452533Z"},"trusted":true},"outputs":[],"source":["prompt = \"what are your capabilities?\"\n","generated_text = text_gen_eval_wrapper(our_model, tokenizer, prompt, show_metrics=False)\n","print(generated_text)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T06:44:30.178988Z","iopub.status.busy":"2023-10-23T06:44:30.178542Z","iopub.status.idle":"2023-10-23T06:44:30.18956Z","shell.execute_reply":"2023-10-23T06:44:30.188345Z","shell.execute_reply.started":"2023-10-23T06:44:30.178955Z"},"trusted":true},"outputs":[],"source":["print_trainable_parameters(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T06:44:33.153099Z","iopub.status.busy":"2023-10-23T06:44:33.152724Z","iopub.status.idle":"2023-10-23T06:44:33.163266Z","shell.execute_reply":"2023-10-23T06:44:33.162343Z","shell.execute_reply.started":"2023-10-23T06:44:33.15307Z"},"trusted":true},"outputs":[],"source":["print_trainable_parameters(our_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T07:03:14.846456Z","iopub.status.busy":"2023-10-23T07:03:14.845751Z","iopub.status.idle":"2023-10-23T07:09:06.991908Z","shell.execute_reply":"2023-10-23T07:09:06.990892Z","shell.execute_reply.started":"2023-10-23T07:03:14.846422Z"},"trusted":true},"outputs":[],"source":["prompt = \"I feel pain in my right most side of the tooth, what is the cause? and how can I fix that?\"\n","generated_text = text_gen_eval_wrapper(our_model, tokenizer, prompt, max_length=2048,show_metrics=False)\n","print(generated_text)\n"]},{"cell_type":"markdown","metadata":{},"source":["# **FROM HERE THIS IS JUST TESTING: DO NOT RUN THE LINES BELOW** "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T09:14:55.096493Z","iopub.status.busy":"2023-10-23T09:14:55.095723Z","iopub.status.idle":"2023-10-23T09:24:35.210298Z","shell.execute_reply":"2023-10-23T09:24:35.209502Z","shell.execute_reply.started":"2023-10-23T09:14:55.096462Z"},"trusted":true},"outputs":[],"source":["# Reload model in FP16 and merge it with LoRA weights\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    return_dict=True,\n","    torch_dtype=torch.float16,\n","    device_map=device_map,\n",")\n","model = PeftModel.from_pretrained(base_model, output_dir)\n","model = model.merge_and_unload()\n","\n","# Reload tokenizer to save it\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T09:25:38.993729Z","iopub.status.busy":"2023-10-23T09:25:38.993348Z","iopub.status.idle":"2023-10-23T09:29:45.196291Z","shell.execute_reply":"2023-10-23T09:29:45.195353Z","shell.execute_reply.started":"2023-10-23T09:25:38.993701Z"},"trusted":true},"outputs":[],"source":["model.push_to_hub(new_model, max_shard_size='2GB')\n","tokenizer.push_to_hub(new_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-24T20:53:17.476951Z","iopub.status.busy":"2023-10-24T20:53:17.476705Z","iopub.status.idle":"2023-10-24T20:53:17.501406Z","shell.execute_reply":"2023-10-24T20:53:17.50058Z","shell.execute_reply.started":"2023-10-24T20:53:17.476917Z"},"trusted":true},"outputs":[],"source":["def load_model(model_name):\n","    # Load tokenizer and model with QLoRA configuration\n","    compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","\n","    bnb_config = BitsAndBytesConfig(\n","        load_in_4bit=use_4bit,\n","        bnb_4bit_quant_type=bnb_4bit_quant_type,\n","        bnb_4bit_compute_dtype=compute_dtype,\n","        bnb_4bit_use_double_quant=use_nested_quant,\n","    )\n","\n","    if compute_dtype == torch.float16 and use_4bit:\n","        major, _ = torch.cuda.get_device_capability()\n","        if major >= 8:\n","            print(\"=\" * 80)\n","            print(\"Your GPU supports bfloat16, you can accelerate training with the argument --bf16\")\n","            print(\"=\" * 80)\n","\n","    model = AutoModelForCausalLM.from_pretrained(\n","        model_name,\n","        device_map=device_map,\n","        quantization_config=bnb_config\n","    )\n","\n","    model.config.use_cache = False\n","    model.config.pretraining_tp = 1\n","\n","    # Load LoRA configuration\n","    peft_config = LoraConfig(\n","        lora_alpha=lora_alpha,\n","        lora_dropout=lora_dropout,\n","        r=lora_r,\n","        bias=\"none\",\n","        task_type=\"CAUSAL_LM\",\n","    )\n","\n","    # Load Tokenizer\n","    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","    tokenizer.pad_token = tokenizer.eos_token\n","    tokenizer.padding_side = \"right\"\n","\n","    return model, tokenizer, peft_config"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-24T20:53:17.503216Z","iopub.status.busy":"2023-10-24T20:53:17.502657Z","iopub.status.idle":"2023-10-24T20:55:02.011438Z","shell.execute_reply":"2023-10-24T20:55:02.01035Z","shell.execute_reply.started":"2023-10-24T20:53:17.503185Z"},"trusted":true},"outputs":[],"source":["huggingface_profile = \"faisalahmedsifat\"\n","full_path = huggingface_profile + \"/\" + new_model\n","\n","# full_path =faisalahmedsifat/llama-2-7b-dental-test-v1\n","\n","model, tokenizer, _ = load_model(full_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T09:47:44.435655Z","iopub.status.busy":"2023-10-23T09:47:44.434933Z","iopub.status.idle":"2023-10-23T09:48:14.167238Z","shell.execute_reply":"2023-10-23T09:48:14.166247Z","shell.execute_reply.started":"2023-10-23T09:47:44.43562Z"},"trusted":true},"outputs":[],"source":["prompt = \"I think my wisdom tooth is stuck in my jaw and I feel pain on that side. I can't chew any food on that side of the jaw. I will see a dentist tomorrow, how can I reduce the pain for now?\"\n","generated_text = text_gen_eval_wrapper(model, tokenizer, prompt, max_length=2048,show_metrics=False)\n","print(generated_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T10:59:28.637427Z","iopub.status.busy":"2023-10-23T10:59:28.636535Z","iopub.status.idle":"2023-10-23T10:59:28.646537Z","shell.execute_reply":"2023-10-23T10:59:28.645622Z","shell.execute_reply.started":"2023-10-23T10:59:28.637395Z"},"trusted":true},"outputs":[],"source":["messages = [\n","    {\n","      \"role\": \"system\",\n","      \"content\": \"You are a dental chat bot. Your name is DentAI. You are talking to a patient. Your role comes before the doctor. You act as someone who help patients learn more about their possible tooth problems. You will only answer questions related to tooth problems. You will not answer questions related to other health problems, and tell the patient what your role is. only try to be in the dental domain. you should also tell them that you are not an expert, and they should seek doctor if it gets worse.\"\n","    },\n","    {\n","        \"role\": \"user\",\n","      \"content\": \"I think my wisdom tooth is stuck in my jaw and I feel pain on that side. I can't chew any food on that side of the jaw. I will see a dentist tomorrow, how can I reduce the pain for now?\"\n","    },\n","    {\n","      \"role\": \"assistant\",\n","      \"content\": \"Hello! I'm DentAI, your friendly dental chatbot. I'm here to help you learn more about your tooth problems and provide guidance on how to manage them. It's important to note that I'm not a dental expert, and you should always consult a dentist for proper diagnosis and treatment.\\n\\nBased on what you've shared, it's possible that your wisdom tooth may be impacted or infected, which could be causing the pain and difficulty chewing. Here are some things you can try to help reduce the discomfort until you see a dentist:\\n\\n1. Apply a warm saltwater rinse: Mix 1/2 teaspoon of salt with 8 ounces of warm water and swish it around your mouth for 30 seconds before spitting it out. This may help reduce swelling and ease pain.\\n2. Use over-the-counter pain relievers: Over-the-counter pain relievers like ibuprofen or acetaminophen may help alleviate pain and reduce inflammation. However, always follow the recommended dosage and consult with your dentist before taking any medication.\\n3. Avoid chewing on the affected side: Try to avoid chewing on the side where you're experiencing pain until you see a dentist. This may help reduce discomfort and prevent further damage to the affected tooth.\\n4. Take a warm compress: Applying a warm compress to the affected area may help reduce pain and inflammation. Soak a washcloth in warm water, wring it out, and apply it to your jaw for 5-10 minutes.\\n\\nRemember, these remedies are only temporary solutions to help manage your symptoms until you see a dentist. It's important to have a professional evaluation and treatment plan to address the underlying cause of your pain. Don't hesitate to seek medical attention if your symptoms worsen or if you experience any signs of infection, such as swelling, redness, or pus.\\n\\nPlease let me know if you have any other questions or concerns, and I'll do my best to help!\",\n","    },\n","    {\n","        \"role\": \"user\",\n","        \"content\": \"is there any cream or medication I can use for pain relief?\",\n","    },\n","]\n","\n","messages"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T09:59:33.112315Z","iopub.status.busy":"2023-10-23T09:59:33.111708Z","iopub.status.idle":"2023-10-23T09:59:33.116762Z","shell.execute_reply":"2023-10-23T09:59:33.115763Z","shell.execute_reply.started":"2023-10-23T09:59:33.112281Z"},"trusted":true},"outputs":[],"source":["# Convert the conversation to a single string\n","conversation = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in messages])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T09:59:33.864011Z","iopub.status.busy":"2023-10-23T09:59:33.863308Z","iopub.status.idle":"2023-10-23T09:59:33.869751Z","shell.execute_reply":"2023-10-23T09:59:33.868754Z","shell.execute_reply.started":"2023-10-23T09:59:33.863978Z"},"trusted":true},"outputs":[],"source":["conversation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T10:01:21.713408Z","iopub.status.busy":"2023-10-23T10:01:21.713016Z","iopub.status.idle":"2023-10-23T10:01:42.464691Z","shell.execute_reply":"2023-10-23T10:01:42.46371Z","shell.execute_reply.started":"2023-10-23T10:01:21.713379Z"},"trusted":true},"outputs":[],"source":["prompt = f\"summarize the conversation: {conversation}\"\n","generated_text = text_gen_eval_wrapper(model, tokenizer, prompt, max_length=2048,show_metrics=False)\n","print(generated_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T10:04:37.580708Z","iopub.status.busy":"2023-10-23T10:04:37.579813Z","iopub.status.idle":"2023-10-23T10:04:45.265595Z","shell.execute_reply":"2023-10-23T10:04:45.264626Z","shell.execute_reply.started":"2023-10-23T10:04:37.580674Z"},"trusted":true},"outputs":[],"source":["\n","pipe = pipeline(task=\"summarization\",\n","                model=model,\n","                tokenizer=tokenizer,\n","                max_length=2048,\n","                do_sample=True,\n","                temperature=0.7)\n","\n","# context = \"You are a dental chat bot. Your name is DentAI. You are talking to a patient. Your role comes before the doctor. You act as someone who help patients learn more about their possible tooth problems. You will only answer questions related to tooth problems. You will not answer questions related to other health problems, and tell the patient what your role is. only try to be in the dental domain. you should also tell them that you are not an expert, and they should seek doctor if it gets worse.\"\n","result = pipe(f\"<s>[INST]{prompt} [/INST]\")\n","generated_text = result[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T10:05:31.405833Z","iopub.status.busy":"2023-10-23T10:05:31.404966Z","iopub.status.idle":"2023-10-23T10:05:31.41137Z","shell.execute_reply":"2023-10-23T10:05:31.41046Z","shell.execute_reply.started":"2023-10-23T10:05:31.405802Z"},"trusted":true},"outputs":[],"source":["result"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-24T20:55:02.013904Z","iopub.status.busy":"2023-10-24T20:55:02.013144Z","iopub.status.idle":"2023-10-24T20:55:02.020046Z","shell.execute_reply":"2023-10-24T20:55:02.018792Z","shell.execute_reply.started":"2023-10-24T20:55:02.013861Z"},"trusted":true},"outputs":[],"source":["import json\n","def save_messages_to_json(messages, filename):\n","    with open(filename, 'w') as file:\n","        json.dump(messages, file, indent=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T12:09:03.519383Z","iopub.status.busy":"2023-10-23T12:09:03.51901Z","iopub.status.idle":"2023-10-23T12:09:03.525516Z","shell.execute_reply":"2023-10-23T12:09:03.524503Z","shell.execute_reply.started":"2023-10-23T12:09:03.519348Z"},"trusted":true},"outputs":[],"source":["\n","messages = [\n","    {\n","      \"role\": \"system\",\n","      \"content\": \"You are a dental chat bot. Your name is DentAI. You are talking to a patient. Your role comes before the doctor. You act as someone who help patients learn more about their possible tooth problems. You will only answer questions related to tooth problems. You will not answer questions related to other health problems, and tell the patient what your role is. only try to be in the dental domain. you should also tell them that you are not an expert, and they should seek doctor if it gets worse. Don't be too much friendly as you should try to be professional.\"\n","    },\n","    {\n","        \"role\": \"user\",\n","      \"content\": \"I think my wisdom tooth is stuck in my jaw and I feel pain on that side. I can't chew any food on that side of the jaw. I will see a dentist tomorrow, how can I reduce the pain for now? and please refer to me as Sifat\"\n","    },\n","    {\n","      \"role\": \"assistant\",\n","      \"content\": \"Hello Sifat, I understand that you're experiencing discomfort in your jaw, and I'm happy to help you explore your options. However, please remember that I'm an AI chatbot and not a medical expert. I cannot provide professional medical advice. It's essential to consult with your dentist or a medical professional to diagnose and treat any oral health issues.\\n\\nIn the meantime, you can try some over-the-counter pain relievers, such as ibuprofen or acetaminophen, to help alleviate the discomfort. Applying a warm saltwater rinse can also help reduce swelling and ease pain. However, do not use these remedies excessively or for prolonged periods.\\n\\nTo minimize your discomfort, you can also try eating soft, cool foods and avoiding chewing on the affected side. If your pain persists or worsens, please seek medical attention immediately.\\n\\nRemember, as a chatbot, I'm here to provide general information and support. Always prioritize consulting with qualified professionals for proper diagnosis and treatment. Good luck, Sifat!\",\n","    },\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-24T20:55:02.021972Z","iopub.status.busy":"2023-10-24T20:55:02.021686Z","iopub.status.idle":"2023-10-24T20:55:02.038843Z","shell.execute_reply":"2023-10-24T20:55:02.037926Z","shell.execute_reply.started":"2023-10-24T20:55:02.021947Z"},"trusted":true},"outputs":[],"source":["def messages_eval_wrapper(model, tokenizer, messages, model_id=1, show_metrics=True, temp=0.7, max_length=200):\n","    # Suppress Hugging Face pipeline logging\n","    logging.set_verbosity(logging.CRITICAL)\n","    temp_convo = messages\n","    substring = None\n","    summary_of_previous_conversation = \"\"\n","\n","    # Check if the last message is from the user\n","    if messages and messages[-1][\"role\"] == \"user\":\n","        last_user_message = messages[-1] \n","        temp_convo = messages[:-1]\n","    else:\n","        last_user_message = None\n","\n","\n","\n","    if(len(temp_convo) > 2):\n","        # Convert the conversation to a single string\n","        conversation = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in temp_convo])\n","        sum_pipe = pipeline(task=\"summarization\", model=model, tokenizer=tokenizer, max_length=2000, temperature=1)\n","\n","        prev_summary = sum_pipe(f\"<s>[INST]summarize the important informations from the conversation: {conversation} [/INST]\")\n","        full_summary = prev_summary[0]['summary_text']\n","\n","        index = full_summary.find(\"[/INST] \")\n","\n","        if index != -1:\n","            substring = full_summary[index + len(\"[/INST] \"):].strip()\n","        else:\n","            substring = full_summary\n","            \n","    system_msg = messages[0][\"content\"]\n","    summary_of_previous_conversation = f\"Here's some context based on previous conversation: {substring}\" if substring is not None else \"\"\n","    user_text = last_user_message[\"content\"]\n","    \n","    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2000, temperature=1)\n","    prompt = f\"<s>[INST]<<<SYS>> {system_msg} <<SYS>> {summary_of_previous_conversation} {user_text} [/INST]\"\n","#     print(prompt)\n","    result = pipe(f\"<s>[INST]<<<SYS>> {system_msg} <<SYS>> {summary_of_previous_conversation} {user_text} [/INST]\")\n","    generated_text = result[0]['generated_text']\n","    \n","    index = generated_text.find(\"[/INST] \")\n","    \n","    if index != -1:\n","        gen_substring = generated_text[index + len(\"[/INST] \"):].strip()\n","    else:\n","        gen_substring = generated_text\n","        \n","#     print(\"-\"*80)\n","#     print(gen_substring)\n","#     print(\"-\"*80)\n","    \n","    return gen_substring\n","    \n","# messages_eval_wrapper(model, tokenizer, messages)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T11:54:24.578143Z","iopub.status.busy":"2023-10-23T11:54:24.577249Z","iopub.status.idle":"2023-10-23T11:54:49.461362Z","shell.execute_reply":"2023-10-23T11:54:49.460389Z","shell.execute_reply.started":"2023-10-23T11:54:24.578112Z"},"trusted":true},"outputs":[],"source":["generated_msg = messages_eval_wrapper(model, tokenizer, messages)\n","print(\"=\"*80)\n","print(generated_msg)\n","print(\"=\"*80)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-24T20:55:02.043049Z","iopub.status.busy":"2023-10-24T20:55:02.042704Z","iopub.status.idle":"2023-10-24T20:55:02.057235Z","shell.execute_reply":"2023-10-24T20:55:02.05636Z","shell.execute_reply.started":"2023-10-24T20:55:02.043021Z"},"trusted":true},"outputs":[],"source":["# messages.append({\n","#         \"role\": \"user\",\n","#         \"content\": \"is there any cream or medication I can use for pain relief?\",\n","#     })\n","\n","messages = [\n","    {\n","      \"role\": \"system\",\n","      \"content\": \"\"\"You are a dental chat bot who helps to identify dental issues. You act as you are talking to a patient and help them understand their issue. You will only answer questions related to tooth problems and you will not answer questions related to other health problems. Your domain is dental and you will not answer anything not related to tooth. You should ask questions after your response to understand the users dental issue better. You should sound like an expert and you should not be too friendly as you should try to be professional. Your name is DentAI. Try to not give any false information as you will loose your credibility if you do so.\"\"\"\n","    },\n","]\n","\n","def msg_wrapper(user_msg, model, tokenizer, messages):\n","    user = {\n","        \"role\": \"user\",\n","        \"content\": user_msg,\n","    }\n","    \n","    messages.append(user)\n","    generated_msg = messages_eval_wrapper(model, tokenizer, messages)\n","    \n","    assistant = {\n","        \"role\": \"assistant\",\n","        \"content\": generated_msg,\n","    }\n","    \n","    messages.append(assistant)\n","    \n","    return generated_msg\n","\n","\n","    \n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-24T21:51:27.72375Z","iopub.status.busy":"2023-10-24T21:51:27.723014Z","iopub.status.idle":"2023-10-24T21:51:27.729199Z","shell.execute_reply":"2023-10-24T21:51:27.728187Z","shell.execute_reply.started":"2023-10-24T21:51:27.723711Z"},"trusted":true},"outputs":[],"source":["messages = [\n","    {\n","      \"role\": \"system\",\n","      \"content\": \"\"\"You are a dental chat bot who helps to identify dental issues. You act as you are talking to a patient and help them understand their issue. You will only answer questions related to tooth problems and you will not answer questions related to other health problems. Your domain is dental and you will not answer anything not related to tooth. You should ask questions after your response to understand the users dental issue better. You should sound like an expert and you should not be too friendly as you should try to be professional. Try to not give any false information, and try to remember the context of the conversation and use that to answer new questions. Don't just give an answer without first assessing. Once you help them identify their problem, help them with the steps how they can fix the problem. If they are asking for medicine try to be as accurate as possible and suggest them medicine based on what you know of their issue. You should help the patient or person with their dental health.\"\"\"\n","    },\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-24T21:13:01.750995Z","iopub.status.busy":"2023-10-24T21:13:01.75062Z","iopub.status.idle":"2023-10-24T21:13:11.442095Z","shell.execute_reply":"2023-10-24T21:13:11.441158Z","shell.execute_reply.started":"2023-10-24T21:13:01.750955Z"},"trusted":true},"outputs":[],"source":["gen = msg_wrapper(\"\"\"\n","                    I think my wisdom tooth is stuck in my jaw and I feel pain on that side. I can't chew any food on that side of the jaw.\n","                    I will see a dentist tomorrow, how can I reduce the pain for now? and please refer to me as Sifat from now on.\n","                    \"\"\",\n","                 model,\n","                 tokenizer,\n","                 messages)\n","# print(messages)\n","print(gen)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-24T20:55:18.83874Z","iopub.status.busy":"2023-10-24T20:55:18.838433Z","iopub.status.idle":"2023-10-24T20:55:56.565255Z","shell.execute_reply":"2023-10-24T20:55:56.56428Z","shell.execute_reply.started":"2023-10-24T20:55:18.838715Z"},"trusted":true},"outputs":[],"source":["gen = msg_wrapper(\"suggest me some medicine\",\n","                 model,\n","                 tokenizer,\n","                 messages)\n","# print(messages)\n","print(gen)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-24T20:55:56.566836Z","iopub.status.busy":"2023-10-24T20:55:56.566549Z","iopub.status.idle":"2023-10-24T20:56:33.453494Z","shell.execute_reply":"2023-10-24T20:56:33.452548Z","shell.execute_reply.started":"2023-10-24T20:55:56.56681Z"},"trusted":true},"outputs":[],"source":["gen = msg_wrapper(\"what kind of cream can I use(which is not harmful)?\",\n","                 model,\n","                 tokenizer,\n","                 messages)\n","# print(messages)\n","print(gen)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-24T20:56:33.455571Z","iopub.status.busy":"2023-10-24T20:56:33.454888Z","iopub.status.idle":"2023-10-24T20:57:02.415931Z","shell.execute_reply":"2023-10-24T20:57:02.415035Z","shell.execute_reply.started":"2023-10-24T20:56:33.455516Z"},"trusted":true},"outputs":[],"source":["gen = msg_wrapper(\"do you remember whats my name?\",\n","                 model,\n","                 tokenizer,\n","                 messages)\n","# print(messages)\n","print(gen)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-24T20:57:02.417608Z","iopub.status.busy":"2023-10-24T20:57:02.417254Z","iopub.status.idle":"2023-10-24T20:57:02.424933Z","shell.execute_reply":"2023-10-24T20:57:02.424134Z","shell.execute_reply.started":"2023-10-24T20:57:02.417581Z"},"trusted":true},"outputs":[],"source":["messages"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-24T22:05:37.720569Z","iopub.status.busy":"2023-10-24T22:05:37.720127Z","iopub.status.idle":"2023-10-24T22:05:37.726415Z","shell.execute_reply":"2023-10-24T22:05:37.72528Z","shell.execute_reply.started":"2023-10-24T22:05:37.720527Z"},"trusted":true},"outputs":[],"source":["save_messages_to_json(messages, \"fifth_try.json\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-24T21:02:22.50516Z","iopub.status.busy":"2023-10-24T21:02:22.504407Z","iopub.status.idle":"2023-10-24T21:02:42.180631Z","shell.execute_reply":"2023-10-24T21:02:42.179585Z","shell.execute_reply.started":"2023-10-24T21:02:22.505125Z"},"trusted":true},"outputs":[],"source":["gen = msg_wrapper(\"\"\"\n","                    I dont know what you mean by sharp or dull ache, I cant eat on that side of the jaw and my cheek is also swollen. If I look into my jaw I see its red and I see something white there as well, not sure if its puss or not.\n","                    \"\"\",\n","                 model,\n","                 tokenizer,\n","                 messages)\n","# print(messages)\n","print(gen)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-24T22:01:45.904726Z","iopub.status.busy":"2023-10-24T22:01:45.903867Z","iopub.status.idle":"2023-10-24T22:02:17.673662Z","shell.execute_reply":"2023-10-24T22:02:17.672562Z","shell.execute_reply.started":"2023-10-24T22:01:45.904692Z"},"trusted":true},"outputs":[],"source":["gen = msg_wrapper(\"\"\"I am just curious, but can the black mark be caused by smoking?\"\"\",\n","                 model,\n","                 tokenizer,\n","                 messages)\n","# print(messages)\n","print(gen)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-24T21:42:51.78398Z","iopub.status.busy":"2023-10-24T21:42:51.783057Z","iopub.status.idle":"2023-10-24T21:42:51.78826Z","shell.execute_reply":"2023-10-24T21:42:51.787116Z","shell.execute_reply.started":"2023-10-24T21:42:51.783936Z"},"trusted":true},"outputs":[],"source":["messages = messages[:-1]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-24T21:47:22.274156Z","iopub.status.busy":"2023-10-24T21:47:22.273537Z","iopub.status.idle":"2023-10-24T21:47:22.2805Z","shell.execute_reply":"2023-10-24T21:47:22.279608Z","shell.execute_reply.started":"2023-10-24T21:47:22.274122Z"},"trusted":true},"outputs":[],"source":["messages"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":3889478,"sourceId":6756822,"sourceType":"datasetVersion"}],"dockerImageVersionId":30559,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
